# Default configuration values
# This file is loaded when no other configuration is found

api:
  base_url: "http://localhost:12434/engines/llama.cpp/v1/"
  key: "anything"
  timeout: 30
  
  # Client configuration defaults
  client:
    timeout: 30.0          # Default timeout for requests
    max_retries: 2         # Default number of retries
    connection_check: true # Check server health by default
  
  models:
    default: "ai/gemma3"
    available:
      - "ai/gemma3" 
      - "ai/qwen3"
    
    defaults:
      max_tokens: 500
      temperature: 0.7
      top_p: 0.9
      stream: true

logging:
  level: "INFO"
  debug: false

# Error handling defaults
error_handling:
  show_detailed_errors: true
  suggest_fixes: true
  retry_on_timeout: true

ui:
  show_model_info: true
  stream_responses: true
  max_history: 100