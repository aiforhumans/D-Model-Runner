# Local Development Environment Variables for D-Model-Runner
# Use this for local development with Docker Model Runner

# Docker Model Runner OpenAI-compatible endpoint
OPENAI_BASE_URL=http://localhost:12434/engines/llama.cpp/v1/

# Disable streaming for compatibility
OPENAI_DISABLE_STREAMING=true

# Default model to use
OPENAI_MODEL=ai/gemma3

# Alternative models you can switch to:
# OPENAI_MODEL=ai/qwen3
# OPENAI_MODEL=index.docker.io/ai/qwen2.5:7B-F16

# Optional: API key (not required for local Docker Model Runner)
# OPENAI_API_KEY=anything